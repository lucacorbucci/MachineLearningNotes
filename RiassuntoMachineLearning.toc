\contentsline {chapter}{\numberline {1}Lezione 1 - Knn}{9}{chapter.1}% 
\contentsline {section}{\numberline {1.1}K-nn}{11}{section.1.1}% 
\contentsline {section}{\numberline {1.2}Formula per il K-nn}{13}{section.1.2}% 
\contentsline {subsubsection}{Cost sensitive Learning}{14}{section*.7}% 
\contentsline {subsubsection}{Weighted K-nn}{14}{section*.8}% 
\contentsline {subsubsection}{K-nn per regressione}{15}{section*.11}% 
\contentsline {subsubsection}{Come scegliere la K migliore?}{16}{section*.13}% 
\contentsline {subsubsection}{Misure di performance}{18}{section*.16}% 
\contentsline {subsubsection}{Distance Measure}{23}{section*.24}% 
\contentsline {subsubsection}{Interpretazione probabilistica di K-nn}{26}{section*.30}% 
\contentsline {subsection}{\numberline {1.2.1}Problemi noti}{28}{subsection.1.2.1}% 
\contentsline {subsubsection}{Scaling Issue}{28}{section*.35}% 
\contentsline {subsubsection}{Curse of Dimensionality}{30}{section*.38}% 
\contentsline {subsection}{\numberline {1.2.2}Considerazioni su K-nn}{30}{subsection.1.2.2}% 
\contentsline {chapter}{\numberline {2}Decision Tree}{32}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Introduzione}{32}{section.2.1}% 
\contentsline {section}{\numberline {2.2}Inferenza con i decision tree}{36}{section.2.2}% 
\contentsline {section}{\numberline {2.3}Creare il decision Tree ottimo}{38}{section.2.3}% 
\contentsline {subsubsection}{Algoritmo Greedy}{39}{section*.46}% 
\contentsline {chapter}{\numberline {3}Lezione 3 - Probabilistic Inference}{40}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Refresh: }{40}{section.3.1}% 
\contentsline {section}{\numberline {3.2}MLE: Ottimizzare la funzione Likelihood}{41}{section.3.2}% 
\contentsline {subsection}{\numberline {3.2.1}Ottimizzare}{44}{subsection.3.2.1}% 
\contentsline {subsection}{\numberline {3.2.2}Problemi della MLE}{46}{subsection.3.2.2}% 
\contentsline {section}{\numberline {3.3}Bayesian Inference}{47}{section.3.3}% 
\contentsline {subsection}{\numberline {3.3.1}Bayes Formula}{49}{subsection.3.3.1}% 
\contentsline {subsection}{\numberline {3.3.2}Problema}{51}{subsection.3.3.2}% 
\contentsline {section}{\numberline {3.4}MAP: Maximum A Posterior Estimation}{52}{section.3.4}% 
\contentsline {subsection}{\numberline {3.4.1}Scegliere la prior distribution}{52}{subsection.3.4.1}% 
\contentsline {subsection}{\numberline {3.4.2}Calcolare la MAP}{54}{subsection.3.4.2}% 
\contentsline {section}{\numberline {3.5}Estimate the posterior distribution}{56}{section.3.5}% 
\contentsline {subsection}{\numberline {3.5.1}Vantaggi del calcolo completo della posterior distribution}{59}{subsection.3.5.1}% 
\contentsline {subsubsection}{Soluzioni viste fino ad ora}{59}{section*.73}% 
\contentsline {subsubsection}{Visualizzare la posterior distribution}{60}{section*.75}% 
\contentsline {section}{\numberline {3.6}Previsione del prossimo lancio}{61}{section.3.6}% 
\contentsline {chapter}{\numberline {4}Linear Regression}{65}{chapter.4}% 
\contentsline {section}{\numberline {4.1}Loss Function}{66}{section.4.1}% 
\contentsline {section}{\numberline {4.2}NonLinear Dependency in data}{68}{section.4.2}% 
\contentsline {section}{\numberline {4.3}Come scegliere il grado del polinomio}{72}{section.4.3}% 
\contentsline {subsection}{\numberline {4.3.1}Regularization}{74}{subsection.4.3.1}% 
\contentsline {section}{\numberline {4.4}Bias - Variance tradeoff}{76}{section.4.4}% 
\contentsline {section}{\numberline {4.5}Probabilistic Linear Regression}{79}{section.4.5}% 
\contentsline {subsection}{\numberline {4.5.1}Bayesian Network}{79}{subsection.4.5.1}% 
\contentsline {section}{\numberline {4.6}Probabilistic Formulation of Linear Regression}{81}{section.4.6}% 
\contentsline {subsection}{\numberline {4.6.1}Calcolare la Likelihood}{82}{subsection.4.6.1}% 
\contentsline {subsection}{\numberline {4.6.2}Calcolare la posterior Distribution}{85}{subsection.4.6.2}% 
\contentsline {subsubsection}{Scegliere la prior distribution}{86}{section*.128}% 
\contentsline {subsubsection}{Massimizzare la posterior distribution: MAP}{87}{section*.130}% 
\contentsline {subsection}{\numberline {4.6.3}Fully Bayesian Approach}{88}{subsection.4.6.3}% 
\contentsline {subsection}{\numberline {4.6.4}Sequential Bayesian Linear Regression}{90}{subsection.4.6.4}% 
\contentsline {subsubsection}{Un esempio di Sequential Bayesian Linear Regression}{90}{section*.140}% 
\contentsline {subsection}{\numberline {4.6.5}Fare previsioni su nuovi dati}{93}{subsection.4.6.5}% 
\contentsline {chapter}{\numberline {5}Linear Classification}{97}{chapter.5}% 
\contentsline {subsection}{\numberline {5.0.1}Come capire se le previsioni sono corrette?}{98}{subsection.5.0.1}% 
\contentsline {subsection}{\numberline {5.0.2}Usare l'iperpiano come decision boundary}{99}{subsection.5.0.2}% 
\contentsline {subsubsection}{Multiple Classes}{101}{section*.155}% 
\contentsline {subsection}{\numberline {5.0.3}Least Squares per la classificazione}{102}{subsection.5.0.3}% 
\contentsline {subsection}{\numberline {5.0.4}Perceptron Algorithm}{103}{subsection.5.0.4}% 
\contentsline {section}{\numberline {5.1}Classi non linearmente separabili}{106}{section.5.1}% 
\contentsline {subsection}{\numberline {5.1.1}Altre limitazioni}{107}{subsection.5.1.1}% 
\contentsline {section}{\numberline {5.2}Probabilistic Model}{108}{section.5.2}% 
\contentsline {subsection}{\numberline {5.2.1}Generative Model}{110}{subsection.5.2.1}% 
\contentsline {subsubsection}{Scegliere la prior distribution}{111}{section*.174}% 
\contentsline {subsubsection}{Class conditionals}{114}{section*.182}% 
\contentsline {subsubsection}{Posterior distribution}{116}{section*.189}% 
\contentsline {subsection}{\numberline {5.2.2}LDA - Linear Discriminant Analysis}{117}{subsection.5.2.2}% 
\contentsline {subsubsection}{Estensione a pi\IeC {\`u} classi}{119}{section*.199}% 
\contentsline {subsubsection}{Softmax function}{120}{section*.203}% 
\contentsline {subsection}{\numberline {5.2.3}Naive Bayes}{121}{subsection.5.2.3}% 
\contentsline {section}{\numberline {5.3}Probabilistic Discriminative Models for linear classification}{123}{section.5.3}% 
\contentsline {subsection}{\numberline {5.3.1}Logistic Regression}{124}{subsection.5.3.1}% 
\contentsline {subsection}{\numberline {5.3.2}Likelihood della logistic Regression}{126}{subsection.5.3.2}% 
\contentsline {subsection}{\numberline {5.3.3}Logistic Regressione e Weights regularization}{127}{subsection.5.3.3}% 
\contentsline {subsection}{\numberline {5.3.4}Multiclass logistic regression}{128}{subsection.5.3.4}% 
\contentsline {subsubsection}{Loss nel caso della Multiclass logistic regression}{128}{section*.222}% 
\contentsline {section}{\numberline {5.4}Conclusioni}{129}{section.5.4}% 
\contentsline {chapter}{\numberline {6}Optimization}{130}{chapter.6}% 
\contentsline {subsection}{\numberline {6.0.1}Convexity}{132}{subsection.6.0.1}% 
\contentsline {subsubsection}{Convex Set}{132}{section*.226}% 
\contentsline {subsubsection}{Convex Function}{132}{section*.228}% 
\contentsline {subsection}{\numberline {6.0.2}Propriet\IeC {\`a} delle convex function}{133}{subsection.6.0.2}% 
\contentsline {subsection}{\numberline {6.0.3}First Order Convexity Condition}{133}{subsection.6.0.3}% 
\contentsline {subsection}{\numberline {6.0.4}Vertex}{136}{subsection.6.0.4}% 
\contentsline {subsection}{\numberline {6.0.5}Convex hull}{136}{subsection.6.0.5}% 
\contentsline {section}{\numberline {6.1}Maximization Problem}{137}{section.6.1}% 
\contentsline {subsubsection}{Non con vex Set}{139}{section*.243}% 
\contentsline {subsection}{\numberline {6.1.1}Come verificare che una funzione \IeC {\`e} convessa}{139}{subsection.6.1.1}% 
\contentsline {subsection}{\numberline {6.1.2}Verificare che un set \IeC {\`e} convesso}{140}{subsection.6.1.2}% 
\contentsline {subsection}{\numberline {6.1.3}Problemi semplici}{141}{subsection.6.1.3}% 
\contentsline {subsection}{\numberline {6.1.4}Coordinate descent}{141}{subsection.6.1.4}% 
\contentsline {section}{\numberline {6.2}Gradient Descent}{142}{section.6.2}% 
\contentsline {subsubsection}{Convergenza}{143}{section*.251}% 
\contentsline {section}{\numberline {6.3}Line search}{144}{section.6.3}% 
\contentsline {subsubsection}{Note}{146}{section*.257}% 
\contentsline {subsubsection}{Parallel implementation}{146}{section*.259}% 
\contentsline {subsection}{\numberline {6.3.1}Un algoritmo pi\IeC {\`u} veloce}{147}{subsection.6.3.1}% 
\contentsline {subsubsection}{Possibili soluzioni per la scelta del learning rate}{149}{section*.264}% 
\contentsline {subsubsection}{Adam: Adaptive Moment Estimation}{150}{section*.267}% 
\contentsline {section}{\numberline {6.4}Discussione e altri metodi}{151}{section.6.4}% 
\contentsline {subsection}{\numberline {6.4.1}Metodo di Newton}{152}{subsection.6.4.1}% 
\contentsline {subsubsection}{Note}{153}{section*.275}% 
\contentsline {subsection}{\numberline {6.4.2}Stochastic Gradient Descent}{154}{subsection.6.4.2}% 
\contentsline {subsubsection}{Funzionamento}{154}{section*.276}% 
\contentsline {subsection}{\numberline {6.4.3}Esempio con il Perceptron}{155}{subsection.6.4.3}% 
\contentsline {subsubsection}{Risolviamo con SGD}{157}{section*.285}% 
\contentsline {subsection}{\numberline {6.4.4}Ottimizzazione per la logistic regression}{158}{subsection.6.4.4}% 
\contentsline {subsection}{\numberline {6.4.5}Distributed Learning}{158}{subsection.6.4.5}% 
\contentsline {subsubsection}{Full gradient descent}{159}{section*.290}% 
\contentsline {subsubsection}{Stochastic Gradient Descent}{160}{section*.293}% 
\contentsline {chapter}{\numberline {7}Constrained Optimization}{161}{chapter.7}% 
\contentsline {subsection}{\numberline {7.0.1}Problemi standard}{162}{subsection.7.0.1}% 
\contentsline {subsubsection}{Linear Programming}{162}{section*.298}% 
\contentsline {subsubsection}{Quadratic Programming}{163}{section*.301}% 
\contentsline {section}{\numberline {7.1}Usare il Gradient Descent per risolvere problemi di Constrained Optimization}{164}{section.7.1}% 
\contentsline {subsection}{\numberline {7.1.1}Calcolare le proiezioni}{166}{subsection.7.1.1}% 
\contentsline {subsection}{\numberline {7.1.2}Note}{167}{subsection.7.1.2}% 
\contentsline {section}{\numberline {7.2}Lagrangian e Duality}{167}{section.7.2}% 
\contentsline {subsection}{\numberline {7.2.1}Single Inequality Constraint}{167}{subsection.7.2.1}% 
\contentsline {subsection}{\numberline {7.2.2}Multiple Inequality Constraints}{171}{subsection.7.2.2}% 
\contentsline {subsubsection}{Lagrangian}{173}{section*.318}% 
\contentsline {subsubsection}{Esempio dal video}{175}{section*.322}% 
\contentsline {subsubsection}{Lagrange dual function}{176}{section*.325}% 
\contentsline {subsubsection}{Duality}{177}{section*.327}% 
\contentsline {subsubsection}{Slater's constraint qualification}{179}{section*.330}% 
\contentsline {subsubsection}{Soluzione di un constrained optimization problem}{180}{section*.332}% 
\contentsline {subsubsection}{KKT Condition}{181}{section*.334}% 
\contentsline {chapter}{\numberline {8}SVM}{183}{chapter.8}% 
\contentsline {section}{\numberline {8.1}Classificazione binaria}{183}{section.8.1}% 
\contentsline {subsubsection}{Iperpiani}{184}{section*.337}% 
\contentsline {section}{\numberline {8.2}SVM con Hard Margin}{186}{section.8.2}% 
\contentsline {subsubsection}{Trovare w e b}{188}{section*.341}% 
\contentsline {subsubsection}{Calcolo del margine come problema di ottimizzazione}{189}{section*.342}% 
\contentsline {subsubsection}{Risolvere il problema di ottimizzazione della SVM}{190}{section*.343}% 
\contentsline {subsubsection}{Risolvere il problema duale}{192}{section*.350}% 
\contentsline {subsubsection}{Trovare w e b dalla soluzione duale}{193}{section*.352}% 
\contentsline {section}{\numberline {8.3}Soft Margin Support Vector Machine}{195}{section.8.3}% 
\contentsline {subsubsection}{Slack Variables}{196}{section*.354}% 
\contentsline {subsubsection}{Hinge Loss}{204}{section*.367}% 
\contentsline {section}{\numberline {8.4}Kernel}{205}{section.8.4}% 
\contentsline {subsubsection}{Kernel Matrix (Gram Matrix)}{208}{section*.379}% 
\contentsline {subsubsection}{Valid Kernel}{209}{section*.382}% 
\contentsline {subsubsection}{Kernel Preserving Operations}{209}{section*.383}% 
\contentsline {subsection}{\numberline {8.4.1}Esempi di Kernel}{209}{subsection.8.4.1}% 
\contentsline {section}{\numberline {8.5}Classificare un nuovo punto con SVM e Kernel}{210}{section.8.5}% 
\contentsline {subsubsection}{Nota importante}{211}{section*.388}% 
\contentsline {section}{\numberline {8.6}SVM con classi multiple}{213}{section.8.6}% 
\contentsline {chapter}{\numberline {9}Deep Learning}{214}{chapter.9}% 
\contentsline {section}{\numberline {9.1}Introduzione}{214}{section.9.1}% 
\contentsline {subsection}{\numberline {9.1.1}Perch\IeC {\`e} usiamo i Deep Feedforward Network?}{215}{subsection.9.1.1}% 
\contentsline {section}{\numberline {9.2}Il dataset dello XOR}{216}{section.9.2}% 
\contentsline {subsection}{\numberline {9.2.1}Esempio}{217}{subsection.9.2.1}% 
\contentsline {subsection}{\numberline {9.2.2}Spiegazione e confronto con Logistic Regression}{220}{subsection.9.2.2}% 
\contentsline {subsubsection}{Trovare la basis function}{223}{section*.402}% 
\contentsline {subsubsection}{Neural Network pi\IeC {\`u} complicate}{225}{section*.405}% 
\contentsline {subsubsection}{Activation Function}{227}{section*.408}% 
\contentsline {subsection}{\numberline {9.2.3}Approfondimento sulle Activation function e sulle loro propriet\IeC {\`a}}{229}{subsection.9.2.3}% 
\contentsline {subsection}{\numberline {9.2.4}Universal Approximation Theorem}{231}{subsection.9.2.4}% 
\contentsline {subsection}{\numberline {9.2.5}Usare pi\IeC {\`u} layer}{232}{subsection.9.2.5}% 
\contentsline {section}{\numberline {9.3}Parameter Learning}{234}{section.9.3}% 
\contentsline {subsubsection}{Esempio 1: Binary classification}{235}{section*.412}% 
\contentsline {subsubsection}{Example 2: Multi-class classification}{236}{section*.414}% 
\contentsline {subsubsection}{Esempio 3: Regressione}{237}{section*.417}% 
\contentsline {section}{\numberline {9.4}Minimizzazione della cost function}{237}{section.9.4}% 
\contentsline {subsection}{\numberline {9.4.1}Come calcoliamo il gradiente?}{238}{subsection.9.4.1}% 
\contentsline {subsubsection}{Jacobian}{241}{section*.421}% 
\contentsline {subsection}{\numberline {9.4.2}Computational Graph}{243}{subsection.9.4.2}% 
\contentsline {subsection}{\numberline {9.4.3}Example of BackPropagation}{246}{subsection.9.4.3}% 
\contentsline {subsection}{\numberline {9.4.4}Computational Graph}{249}{subsection.9.4.4}% 
\contentsline {subsubsection}{Ancora sulla BackPropagation}{252}{section*.437}% 
\contentsline {section}{\numberline {9.5}Note}{253}{section.9.5}% 
\contentsline {chapter}{\numberline {10}Deep Learning 2}{255}{chapter.10}% 
\contentsline {subsection}{\numberline {10.0.1}Altri utilizzi per le NN}{255}{subsection.10.0.1}% 
\contentsline {section}{\numberline {10.1}Layer e CNN}{256}{section.10.1}% 
\contentsline {subsection}{\numberline {10.1.1}CNN}{257}{subsection.10.1.1}% 
\contentsline {subsubsection}{Motivation}{259}{section*.444}% 
\contentsline {subsubsection}{Come \IeC {\`e} definita la CNN}{262}{section*.447}% 
\contentsline {subsubsection}{Un esempio}{263}{section*.450}% 
\contentsline {subsubsection}{Come ci comportiamo sui bordi dell'immagine?}{265}{section*.452}% 
\contentsline {subsubsection}{Strides}{266}{section*.453}% 
\contentsline {subsubsection}{Pooling}{266}{section*.454}% 
\contentsline {subsubsection}{Summary}{268}{section*.456}% 
\contentsline {section}{\numberline {10.2}RNN: Recurrent Neural Networks}{268}{section.10.2}% 
\contentsline {subsection}{\numberline {10.2.1}Unfolding computational Graph}{269}{subsection.10.2.1}% 
\contentsline {subsection}{\numberline {10.2.2}RNN e training}{273}{subsection.10.2.2}% 
\contentsline {subsubsection}{Training}{275}{section*.467}% 
\contentsline {subsubsection}{Weight Sharing}{276}{section*.468}% 
\contentsline {section}{\numberline {10.3}Training Deep Neural Networks}{276}{section.10.3}% 
\contentsline {subsubsection}{Xavier Glorot Initialization}{277}{section*.469}% 
\contentsline {section}{\numberline {10.4}Regularization}{279}{section.10.4}% 
\contentsline {subsubsection}{Ottimizzazione degli iperparametri}{282}{section*.475}% 
\contentsline {section}{\numberline {10.5}Modern Architecture and Tricks}{282}{section.10.5}% 
\contentsline {subsection}{\numberline {10.5.1}Batch Normalization}{282}{subsection.10.5.1}% 
\contentsline {subsection}{\numberline {10.5.2}Attention}{287}{subsection.10.5.2}% 
\contentsline {subsection}{\numberline {10.5.3}Tips and Tricks}{288}{subsection.10.5.3}% 
\contentsline {chapter}{\numberline {11}Dimension Reduction}{290}{chapter.11}% 
\contentsline {subsection}{\numberline {11.0.1}Unsupervised Learning}{290}{subsection.11.0.1}% 
\contentsline {subsection}{\numberline {11.0.2}Dimensionality Reduction}{291}{subsection.11.0.2}% 
\contentsline {subsubsection}{Feature selection}{293}{section*.483}% 
\contentsline {subsubsection}{Linear Transformation}{294}{section*.487}% 
\contentsline {section}{\numberline {11.1}Principal Component Analysis}{298}{section.11.1}% 
\contentsline {subsubsection}{Come troviamo la trasformazione}{299}{section*.491}% 
\contentsline {subsubsection}{Dimensionality Reduction with PCA}{303}{section*.498}% 
\contentsline {subsubsection}{Complessit\IeC {\`a}}{305}{section*.500}% 
\contentsline {subsubsection}{Come si calcolano gli autovettori}{305}{section*.501}% 
\contentsline {section}{\numberline {11.2}Probabilistic PCA}{307}{section.11.2}% 
\contentsline {subsection}{\numberline {11.2.1}Un esempio}{309}{subsection.11.2.1}% 
\contentsline {section}{\numberline {11.3}PPCA: Learning Step}{311}{section.11.3}% 
\contentsline {subsection}{\numberline {11.3.1}Verso la MLE}{311}{subsection.11.3.1}% 
\contentsline {subsection}{\numberline {11.3.2}Usare MLE con la PPCA}{312}{subsection.11.3.2}% 
\contentsline {section}{\numberline {11.4}PPCA: Inference Step}{314}{section.11.4}% 
\contentsline {subsection}{\numberline {11.4.1}PCA e PPCA: pro e contro}{314}{subsection.11.4.1}% 
\contentsline {section}{\numberline {11.5}Singular Value Decomposition: SVD}{316}{section.11.5}% 
\contentsline {section}{\numberline {11.6}Dimensionality Reduction con Low Rank Approximation}{322}{section.11.6}% 
\contentsline {section}{\numberline {11.7}Un'alta interpretazione dell'SVD}{322}{section.11.7}% 
\contentsline {subsection}{\numberline {11.7.1}Approximation}{325}{subsection.11.7.1}% 
\contentsline {subsection}{\numberline {11.7.2}Best Low Rank Approximation - Proof}{327}{subsection.11.7.2}% 
\contentsline {subsection}{\numberline {11.7.3}SVD - Complessit\IeC {\`a}}{327}{subsection.11.7.3}% 
\contentsline {section}{\numberline {11.8}Confronto tra PCA e SVD}{328}{section.11.8}% 
\contentsline {section}{\numberline {11.9}Matrix Factorization}{329}{section.11.9}% 
\contentsline {subsection}{\numberline {11.9.1}L'esempio di Netflix}{329}{subsection.11.9.1}% 
\contentsline {subsubsection}{Alternating Optimization}{332}{section*.531}% 
\contentsline {subsubsection}{Overfitting}{336}{section*.541}% 
\contentsline {subsubsection}{Risolvere l'overfitting con la Regolarizzazione}{336}{section*.542}% 
\contentsline {subsubsection}{L2 vs L1 regularization}{338}{section*.545}% 
\contentsline {section}{\numberline {11.10}Altri metodi per fattorizzare le matrici}{339}{section.11.10}% 
\contentsline {subsubsection}{Boolean Matrix factorization}{340}{section*.548}% 
\contentsline {section}{\numberline {11.11}AutoEncoders}{341}{section.11.11}% 
\contentsline {chapter}{\numberline {12}Clustering}{346}{chapter.12}% 
\contentsline {section}{\numberline {12.1}K-Means}{346}{section.12.1}% 
\contentsline {subsubsection}{Come scegliamo la K?}{349}{section*.555}% 
\contentsline {subsubsection}{KMeans++}{350}{section*.556}% 
\contentsline {subsubsection}{Problemi}{350}{section*.557}% 
\contentsline {section}{\numberline {12.2}Guassian Mixture Model}{351}{section.12.2}% 
\contentsline {subsubsection}{Stimare i parametri}{351}{section*.558}% 
\contentsline {subsubsection}{Gaussian Mixture Model}{352}{section*.559}% 
\contentsline {subsubsection}{Inference Phase}{355}{section*.566}% 
\contentsline {subsubsection}{Learning Phase}{357}{section*.569}% 
\contentsline {section}{\numberline {12.3}Expectation Maximization}{361}{section.12.3}% 
\contentsline {chapter}{\numberline {13}Introduction to PyTorch}{366}{chapter.13}% 
\contentsline {section}{\numberline {13.1}Manually Setup the Affine Layer}{366}{section.13.1}% 
\contentsline {section}{\numberline {13.2}Declare the layer using PyTorch}{367}{section.13.2}% 
\contentsline {subsection}{\numberline {13.2.1}nn.Linear}{368}{subsection.13.2.1}% 
\contentsline {subsection}{\numberline {13.2.2}Torch.optim}{368}{subsection.13.2.2}% 
\contentsline {subsection}{\numberline {13.2.3}DataLoader}{369}{subsection.13.2.3}% 
\contentsline {section}{\numberline {13.3}Validation}{369}{section.13.3}% 
\contentsline {section}{\numberline {13.4}Implement a CNN using PyTorch}{369}{section.13.4}% 
\contentsline {subsection}{\numberline {13.4.1}nn.Sequential}{370}{subsection.13.4.1}% 
\contentsline {chapter}{\numberline {14}Math Refresh}{372}{chapter.14}% 
\contentsline {section}{\numberline {14.1}Loss function}{372}{section.14.1}% 
\contentsline {section}{\numberline {14.2}0-1 Loss}{372}{section.14.2}% 
\contentsline {section}{\numberline {14.3}Lagrange Multipliers}{374}{section.14.3}% 
\contentsline {subsubsection}{Lagrange multiplier con vincoli $\geq $}{375}{section*.592}% 
\contentsline {section}{\numberline {14.4}Probability Distribution}{376}{section.14.4}% 
\contentsline {subsection}{\numberline {14.4.1}Bernoulli Distribution}{377}{subsection.14.4.1}% 
\contentsline {subsection}{\numberline {14.4.2}Normal Distribution}{377}{subsection.14.4.2}% 
\contentsline {subsection}{\numberline {14.4.3}Beta Distribution}{378}{subsection.14.4.3}% 
\contentsline {subsection}{\numberline {14.4.4}Categorical Distribution}{379}{subsection.14.4.4}% 
\contentsline {section}{\numberline {14.5}Propriet\IeC {\`a} dei logaritmi}{379}{section.14.5}% 
\contentsline {section}{\numberline {14.6}Derivate}{379}{section.14.6}% 
\contentsline {section}{\numberline {14.7}Integrali}{380}{section.14.7}% 
\contentsline {section}{\numberline {14.8}Derivate di matrici e Vettori}{381}{section.14.8}% 
