\contentsline {chapter}{\numberline {1}Lezione 1 - Knn}{9}{chapter.1}% 
\contentsline {section}{\numberline {1.1}K-nn}{11}{section.1.1}% 
\contentsline {section}{\numberline {1.2}Formula per il K-nn}{13}{section.1.2}% 
\contentsline {subsubsection}{Cost sensitive Learning}{14}{section*.7}% 
\contentsline {subsubsection}{Weighted K-nn}{14}{section*.8}% 
\contentsline {subsubsection}{K-nn per regressione}{15}{section*.11}% 
\contentsline {subsubsection}{Come scegliere la K migliore?}{16}{section*.13}% 
\contentsline {subsubsection}{Misure di performance}{18}{section*.16}% 
\contentsline {subsubsection}{Distance Measure}{23}{section*.24}% 
\contentsline {subsubsection}{Interpretazione probabilistica di K-nn}{26}{section*.30}% 
\contentsline {subsection}{\numberline {1.2.1}Problemi noti}{28}{subsection.1.2.1}% 
\contentsline {subsubsection}{Scaling Issue}{28}{section*.35}% 
\contentsline {subsubsection}{Curse of Dimensionality}{30}{section*.38}% 
\contentsline {subsection}{\numberline {1.2.2}Considerazioni su K-nn}{30}{subsection.1.2.2}% 
\contentsline {chapter}{\numberline {2}Decision Tree}{32}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Introduzione}{32}{section.2.1}% 
\contentsline {section}{\numberline {2.2}Inferenza con i decision tree}{36}{section.2.2}% 
\contentsline {section}{\numberline {2.3}Creare il decision Tree ottimo}{38}{section.2.3}% 
\contentsline {subsubsection}{Algoritmo Greedy}{39}{section*.46}% 
\contentsline {subsubsection}{Misclassification Error (rate)}{40}{section*.48}% 
\contentsline {subsubsection}{Problemi}{41}{section*.52}% 
\contentsline {subsubsection}{Entropia}{42}{section*.55}% 
\contentsline {subsubsection}{Gini Index}{44}{section*.59}% 
\contentsline {subsubsection}{Confronto tra Entropia, Gini Index e Misclassification Rate}{45}{section*.62}% 
\contentsline {section}{\numberline {2.4}Overfitting}{47}{section.2.4}% 
\contentsline {section}{\numberline {2.5}K-Fold Cross Validation}{50}{section.2.5}% 
\contentsline {subsection}{\numberline {2.5.1}Stopping Criterion}{51}{subsection.2.5.1}% 
\contentsline {subsection}{\numberline {2.5.2}Pro e Contro}{52}{subsection.2.5.2}% 
\contentsline {section}{\numberline {2.6}Confronto con KNN}{53}{section.2.6}% 
\contentsline {chapter}{\numberline {3}Lezione 3 - Probabilistic Inference}{54}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Refresh: }{54}{section.3.1}% 
\contentsline {section}{\numberline {3.2}MLE: Ottimizzare la funzione Likelihood}{55}{section.3.2}% 
\contentsline {subsection}{\numberline {3.2.1}Ottimizzare}{58}{subsection.3.2.1}% 
\contentsline {subsection}{\numberline {3.2.2}Problemi della MLE}{60}{subsection.3.2.2}% 
\contentsline {section}{\numberline {3.3}Bayesian Inference}{61}{section.3.3}% 
\contentsline {subsection}{\numberline {3.3.1}Bayes Formula}{63}{subsection.3.3.1}% 
\contentsline {subsection}{\numberline {3.3.2}Problema}{65}{subsection.3.3.2}% 
\contentsline {section}{\numberline {3.4}MAP: Maximum A Posterior Estimation}{66}{section.3.4}% 
\contentsline {subsection}{\numberline {3.4.1}Scegliere la prior distribution}{66}{subsection.3.4.1}% 
\contentsline {subsection}{\numberline {3.4.2}Calcolare la MAP}{68}{subsection.3.4.2}% 
\contentsline {section}{\numberline {3.5}Estimate the posterior distribution}{70}{section.3.5}% 
\contentsline {subsection}{\numberline {3.5.1}Vantaggi del calcolo completo della posterior distribution}{73}{subsection.3.5.1}% 
\contentsline {subsubsection}{Soluzioni viste fino ad ora}{73}{section*.93}% 
\contentsline {subsubsection}{Visualizzare la posterior distribution}{74}{section*.95}% 
\contentsline {section}{\numberline {3.6}Previsione del prossimo lancio}{75}{section.3.6}% 
\contentsline {chapter}{\numberline {4}Linear Regression}{79}{chapter.4}% 
\contentsline {section}{\numberline {4.1}Loss Function}{80}{section.4.1}% 
\contentsline {section}{\numberline {4.2}NonLinear Dependency in data}{82}{section.4.2}% 
\contentsline {section}{\numberline {4.3}Come scegliere il grado del polinomio}{86}{section.4.3}% 
\contentsline {subsection}{\numberline {4.3.1}Regularization}{88}{subsection.4.3.1}% 
\contentsline {section}{\numberline {4.4}Bias - Variance tradeoff}{90}{section.4.4}% 
\contentsline {section}{\numberline {4.5}Probabilistic Linear Regression}{93}{section.4.5}% 
\contentsline {subsection}{\numberline {4.5.1}Bayesian Network}{93}{subsection.4.5.1}% 
\contentsline {section}{\numberline {4.6}Probabilistic Formulation of Linear Regression}{95}{section.4.6}% 
\contentsline {subsection}{\numberline {4.6.1}Calcolare la Likelihood}{96}{subsection.4.6.1}% 
\contentsline {subsection}{\numberline {4.6.2}Calcolare la posterior Distribution}{99}{subsection.4.6.2}% 
\contentsline {subsubsection}{Scegliere la prior distribution}{100}{section*.148}% 
\contentsline {subsubsection}{Massimizzare la posterior distribution: MAP}{101}{section*.150}% 
\contentsline {subsection}{\numberline {4.6.3}Fully Bayesian Approach}{102}{subsection.4.6.3}% 
\contentsline {subsection}{\numberline {4.6.4}Sequential Bayesian Linear Regression}{104}{subsection.4.6.4}% 
\contentsline {subsubsection}{Un esempio di Sequential Bayesian Linear Regression}{104}{section*.160}% 
\contentsline {subsection}{\numberline {4.6.5}Fare previsioni su nuovi dati}{107}{subsection.4.6.5}% 
\contentsline {chapter}{\numberline {5}Linear Classification}{111}{chapter.5}% 
\contentsline {subsection}{\numberline {5.0.1}Come capire se le previsioni sono corrette?}{112}{subsection.5.0.1}% 
\contentsline {subsection}{\numberline {5.0.2}Usare l'iperpiano come decision boundary}{113}{subsection.5.0.2}% 
\contentsline {subsubsection}{Multiple Classes}{115}{section*.175}% 
\contentsline {subsection}{\numberline {5.0.3}Least Squares per la classificazione}{116}{subsection.5.0.3}% 
\contentsline {subsection}{\numberline {5.0.4}Perceptron Algorithm}{117}{subsection.5.0.4}% 
\contentsline {section}{\numberline {5.1}Classi non linearmente separabili}{120}{section.5.1}% 
\contentsline {subsection}{\numberline {5.1.1}Altre limitazioni}{121}{subsection.5.1.1}% 
\contentsline {section}{\numberline {5.2}Probabilistic Model}{122}{section.5.2}% 
\contentsline {subsection}{\numberline {5.2.1}Generative Model}{124}{subsection.5.2.1}% 
\contentsline {subsubsection}{Scegliere la prior distribution}{125}{section*.194}% 
\contentsline {subsubsection}{Class conditionals}{128}{section*.202}% 
\contentsline {subsubsection}{Posterior distribution}{130}{section*.209}% 
\contentsline {subsection}{\numberline {5.2.2}LDA - Linear Discriminant Analysis}{131}{subsection.5.2.2}% 
\contentsline {subsubsection}{Estensione a pi\IeC {\`u} classi}{133}{section*.219}% 
\contentsline {subsubsection}{Softmax function}{134}{section*.223}% 
\contentsline {subsection}{\numberline {5.2.3}Naive Bayes}{135}{subsection.5.2.3}% 
\contentsline {section}{\numberline {5.3}Probabilistic Discriminative Models for linear classification}{137}{section.5.3}% 
\contentsline {subsection}{\numberline {5.3.1}Logistic Regression}{138}{subsection.5.3.1}% 
\contentsline {subsection}{\numberline {5.3.2}Likelihood della logistic Regression}{140}{subsection.5.3.2}% 
\contentsline {subsection}{\numberline {5.3.3}Logistic Regressione e Weights regularization}{141}{subsection.5.3.3}% 
\contentsline {subsection}{\numberline {5.3.4}Multiclass logistic regression}{142}{subsection.5.3.4}% 
\contentsline {subsubsection}{Loss nel caso della Multiclass logistic regression}{142}{section*.242}% 
\contentsline {section}{\numberline {5.4}Conclusioni}{143}{section.5.4}% 
\contentsline {chapter}{\numberline {6}Optimization}{144}{chapter.6}% 
\contentsline {subsection}{\numberline {6.0.1}Convexity}{146}{subsection.6.0.1}% 
\contentsline {subsubsection}{Convex Set}{146}{section*.246}% 
\contentsline {subsubsection}{Convex Function}{146}{section*.248}% 
\contentsline {subsection}{\numberline {6.0.2}Propriet\IeC {\`a} delle convex function}{147}{subsection.6.0.2}% 
\contentsline {subsection}{\numberline {6.0.3}First Order Convexity Condition}{147}{subsection.6.0.3}% 
\contentsline {subsection}{\numberline {6.0.4}Vertex}{150}{subsection.6.0.4}% 
\contentsline {subsection}{\numberline {6.0.5}Convex hull}{150}{subsection.6.0.5}% 
\contentsline {section}{\numberline {6.1}Maximization Problem}{151}{section.6.1}% 
\contentsline {subsubsection}{Non con vex Set}{153}{section*.263}% 
\contentsline {subsection}{\numberline {6.1.1}Come verificare che una funzione \IeC {\`e} convessa}{153}{subsection.6.1.1}% 
\contentsline {subsection}{\numberline {6.1.2}Verificare che un set \IeC {\`e} convesso}{154}{subsection.6.1.2}% 
\contentsline {subsection}{\numberline {6.1.3}Problemi semplici}{155}{subsection.6.1.3}% 
\contentsline {subsection}{\numberline {6.1.4}Coordinate descent}{155}{subsection.6.1.4}% 
\contentsline {section}{\numberline {6.2}Gradient Descent}{156}{section.6.2}% 
\contentsline {subsubsection}{Convergenza}{157}{section*.271}% 
\contentsline {section}{\numberline {6.3}Line search}{158}{section.6.3}% 
\contentsline {subsubsection}{Note}{160}{section*.277}% 
\contentsline {subsubsection}{Parallel implementation}{160}{section*.279}% 
\contentsline {subsection}{\numberline {6.3.1}Un algoritmo pi\IeC {\`u} veloce}{161}{subsection.6.3.1}% 
\contentsline {subsubsection}{Possibili soluzioni per la scelta del learning rate}{163}{section*.284}% 
\contentsline {subsubsection}{Adam: Adaptive Moment Estimation}{164}{section*.287}% 
\contentsline {section}{\numberline {6.4}Discussione e altri metodi}{165}{section.6.4}% 
\contentsline {subsection}{\numberline {6.4.1}Metodo di Newton}{166}{subsection.6.4.1}% 
\contentsline {subsubsection}{Note}{167}{section*.295}% 
\contentsline {subsection}{\numberline {6.4.2}Stochastic Gradient Descent}{168}{subsection.6.4.2}% 
\contentsline {subsubsection}{Funzionamento}{168}{section*.296}% 
\contentsline {subsection}{\numberline {6.4.3}Esempio con il Perceptron}{169}{subsection.6.4.3}% 
\contentsline {subsubsection}{Risolviamo con SGD}{171}{section*.305}% 
\contentsline {subsection}{\numberline {6.4.4}Ottimizzazione per la logistic regression}{172}{subsection.6.4.4}% 
\contentsline {subsection}{\numberline {6.4.5}Distributed Learning}{172}{subsection.6.4.5}% 
\contentsline {subsubsection}{Full gradient descent}{173}{section*.310}% 
\contentsline {subsubsection}{Stochastic Gradient Descent}{174}{section*.313}% 
\contentsline {chapter}{\numberline {7}Constrained Optimization}{175}{chapter.7}% 
\contentsline {subsection}{\numberline {7.0.1}Problemi standard}{176}{subsection.7.0.1}% 
\contentsline {subsubsection}{Linear Programming}{176}{section*.318}% 
\contentsline {subsubsection}{Quadratic Programming}{177}{section*.321}% 
\contentsline {section}{\numberline {7.1}Usare il Gradient Descent per risolvere problemi di Constrained Optimization}{178}{section.7.1}% 
\contentsline {subsection}{\numberline {7.1.1}Calcolare le proiezioni}{180}{subsection.7.1.1}% 
\contentsline {subsection}{\numberline {7.1.2}Note}{181}{subsection.7.1.2}% 
\contentsline {section}{\numberline {7.2}Lagrangian e Duality}{181}{section.7.2}% 
\contentsline {subsection}{\numberline {7.2.1}Single Inequality Constraint}{181}{subsection.7.2.1}% 
\contentsline {subsection}{\numberline {7.2.2}Multiple Inequality Constraints}{185}{subsection.7.2.2}% 
\contentsline {subsubsection}{Lagrangian}{187}{section*.338}% 
\contentsline {subsubsection}{Esempio dal video}{189}{section*.342}% 
\contentsline {subsubsection}{Lagrange dual function}{190}{section*.345}% 
\contentsline {subsubsection}{Duality}{191}{section*.347}% 
\contentsline {subsubsection}{Slater's constraint qualification}{193}{section*.350}% 
\contentsline {subsubsection}{Soluzione di un constrained optimization problem}{194}{section*.352}% 
\contentsline {subsubsection}{KKT Condition}{195}{section*.354}% 
\contentsline {chapter}{\numberline {8}SVM}{197}{chapter.8}% 
\contentsline {section}{\numberline {8.1}Classificazione binaria}{197}{section.8.1}% 
\contentsline {subsubsection}{Iperpiani}{198}{section*.357}% 
\contentsline {section}{\numberline {8.2}SVM con Hard Margin}{200}{section.8.2}% 
\contentsline {subsubsection}{Trovare w e b}{202}{section*.361}% 
\contentsline {subsubsection}{Calcolo del margine come problema di ottimizzazione}{203}{section*.362}% 
\contentsline {subsubsection}{Risolvere il problema di ottimizzazione della SVM}{204}{section*.363}% 
\contentsline {subsubsection}{Risolvere il problema duale}{206}{section*.370}% 
\contentsline {subsubsection}{Trovare w e b dalla soluzione duale}{207}{section*.372}% 
\contentsline {section}{\numberline {8.3}Soft Margin Support Vector Machine}{209}{section.8.3}% 
\contentsline {subsubsection}{Slack Variables}{210}{section*.374}% 
\contentsline {subsubsection}{Hinge Loss}{218}{section*.387}% 
\contentsline {section}{\numberline {8.4}Kernel}{219}{section.8.4}% 
\contentsline {subsubsection}{Kernel Matrix (Gram Matrix)}{222}{section*.399}% 
\contentsline {subsubsection}{Valid Kernel}{223}{section*.402}% 
\contentsline {subsubsection}{Kernel Preserving Operations}{223}{section*.403}% 
\contentsline {subsection}{\numberline {8.4.1}Esempi di Kernel}{223}{subsection.8.4.1}% 
\contentsline {section}{\numberline {8.5}Classificare un nuovo punto con SVM e Kernel}{224}{section.8.5}% 
\contentsline {subsubsection}{Nota importante}{225}{section*.408}% 
\contentsline {section}{\numberline {8.6}SVM con classi multiple}{227}{section.8.6}% 
\contentsline {chapter}{\numberline {9}Deep Learning}{228}{chapter.9}% 
\contentsline {section}{\numberline {9.1}Introduzione}{228}{section.9.1}% 
\contentsline {subsection}{\numberline {9.1.1}Perch\IeC {\`e} usiamo i Deep Feedforward Network?}{229}{subsection.9.1.1}% 
\contentsline {section}{\numberline {9.2}Il dataset dello XOR}{230}{section.9.2}% 
\contentsline {subsection}{\numberline {9.2.1}Esempio}{231}{subsection.9.2.1}% 
\contentsline {subsection}{\numberline {9.2.2}Spiegazione e confronto con Logistic Regression}{234}{subsection.9.2.2}% 
\contentsline {subsubsection}{Trovare la basis function}{237}{section*.422}% 
\contentsline {subsubsection}{Neural Network pi\IeC {\`u} complicate}{239}{section*.425}% 
\contentsline {subsubsection}{Activation Function}{241}{section*.428}% 
\contentsline {subsection}{\numberline {9.2.3}Approfondimento sulle Activation function e sulle loro propriet\IeC {\`a}}{243}{subsection.9.2.3}% 
\contentsline {subsection}{\numberline {9.2.4}Universal Approximation Theorem}{245}{subsection.9.2.4}% 
\contentsline {subsection}{\numberline {9.2.5}Usare pi\IeC {\`u} layer}{246}{subsection.9.2.5}% 
\contentsline {section}{\numberline {9.3}Parameter Learning}{248}{section.9.3}% 
\contentsline {subsubsection}{Esempio 1: Binary classification}{249}{section*.432}% 
\contentsline {subsubsection}{Example 2: Multi-class classification}{250}{section*.434}% 
\contentsline {subsubsection}{Esempio 3: Regressione}{251}{section*.437}% 
\contentsline {section}{\numberline {9.4}Minimizzazione della cost function}{251}{section.9.4}% 
\contentsline {subsection}{\numberline {9.4.1}Come calcoliamo il gradiente?}{252}{subsection.9.4.1}% 
\contentsline {subsubsection}{Jacobian}{255}{section*.441}% 
\contentsline {subsection}{\numberline {9.4.2}Computational Graph}{257}{subsection.9.4.2}% 
\contentsline {subsection}{\numberline {9.4.3}Example of BackPropagation}{260}{subsection.9.4.3}% 
\contentsline {subsection}{\numberline {9.4.4}Computational Graph}{263}{subsection.9.4.4}% 
\contentsline {subsubsection}{Ancora sulla BackPropagation}{266}{section*.457}% 
\contentsline {section}{\numberline {9.5}Note}{267}{section.9.5}% 
\contentsline {chapter}{\numberline {10}Deep Learning 2}{269}{chapter.10}% 
\contentsline {subsection}{\numberline {10.0.1}Altri utilizzi per le NN}{269}{subsection.10.0.1}% 
\contentsline {section}{\numberline {10.1}Layer e CNN}{270}{section.10.1}% 
\contentsline {subsection}{\numberline {10.1.1}CNN}{271}{subsection.10.1.1}% 
\contentsline {subsubsection}{Motivation}{273}{section*.464}% 
\contentsline {subsubsection}{Come \IeC {\`e} definita la CNN}{276}{section*.467}% 
\contentsline {subsubsection}{Un esempio}{277}{section*.470}% 
\contentsline {subsubsection}{Come ci comportiamo sui bordi dell'immagine?}{279}{section*.472}% 
\contentsline {subsubsection}{Strides}{280}{section*.473}% 
\contentsline {subsubsection}{Pooling}{280}{section*.474}% 
\contentsline {subsubsection}{Summary}{282}{section*.476}% 
\contentsline {section}{\numberline {10.2}RNN: Recurrent Neural Networks}{282}{section.10.2}% 
\contentsline {subsection}{\numberline {10.2.1}Unfolding computational Graph}{283}{subsection.10.2.1}% 
\contentsline {subsection}{\numberline {10.2.2}RNN e training}{287}{subsection.10.2.2}% 
\contentsline {subsubsection}{Training}{289}{section*.487}% 
\contentsline {subsubsection}{Weight Sharing}{290}{section*.488}% 
\contentsline {section}{\numberline {10.3}Training Deep Neural Networks}{290}{section.10.3}% 
\contentsline {subsubsection}{Xavier Glorot Initialization}{291}{section*.489}% 
\contentsline {section}{\numberline {10.4}Regularization}{293}{section.10.4}% 
\contentsline {subsubsection}{Ottimizzazione degli iperparametri}{296}{section*.495}% 
\contentsline {section}{\numberline {10.5}Modern Architecture and Tricks}{296}{section.10.5}% 
\contentsline {subsection}{\numberline {10.5.1}Batch Normalization}{296}{subsection.10.5.1}% 
\contentsline {subsection}{\numberline {10.5.2}Attention}{301}{subsection.10.5.2}% 
\contentsline {subsection}{\numberline {10.5.3}Tips and Tricks}{302}{subsection.10.5.3}% 
\contentsline {chapter}{\numberline {11}Dimension Reduction}{304}{chapter.11}% 
\contentsline {subsection}{\numberline {11.0.1}Unsupervised Learning}{304}{subsection.11.0.1}% 
\contentsline {subsection}{\numberline {11.0.2}Dimensionality Reduction}{305}{subsection.11.0.2}% 
\contentsline {subsubsection}{Feature selection}{307}{section*.503}% 
\contentsline {subsubsection}{Linear Transformation}{308}{section*.507}% 
\contentsline {section}{\numberline {11.1}Principal Component Analysis}{312}{section.11.1}% 
\contentsline {subsubsection}{Come troviamo la trasformazione}{313}{section*.511}% 
\contentsline {subsubsection}{Dimensionality Reduction with PCA}{317}{section*.518}% 
\contentsline {subsubsection}{Complessit\IeC {\`a}}{319}{section*.520}% 
\contentsline {subsubsection}{Come si calcolano gli autovettori}{319}{section*.521}% 
\contentsline {section}{\numberline {11.2}Probabilistic PCA}{321}{section.11.2}% 
\contentsline {subsection}{\numberline {11.2.1}Un esempio}{323}{subsection.11.2.1}% 
\contentsline {section}{\numberline {11.3}Approfondimento sulla Probablisitc PCA}{325}{section.11.3}% 
\contentsline {section}{\numberline {11.4}PPCA: Learning Step}{328}{section.11.4}% 
\contentsline {subsection}{\numberline {11.4.1}Verso la MLE}{328}{subsection.11.4.1}% 
\contentsline {subsection}{\numberline {11.4.2}Usare MLE con la PPCA}{329}{subsection.11.4.2}% 
\contentsline {section}{\numberline {11.5}PPCA: Inference Step}{331}{section.11.5}% 
\contentsline {subsection}{\numberline {11.5.1}PCA e PPCA: pro e contro}{331}{subsection.11.5.1}% 
\contentsline {section}{\numberline {11.6}Singular Value Decomposition: SVD}{333}{section.11.6}% 
\contentsline {section}{\numberline {11.7}Dimensionality Reduction con Low Rank Approximation}{339}{section.11.7}% 
\contentsline {section}{\numberline {11.8}Un'altra interpretazione dell'SVD}{339}{section.11.8}% 
\contentsline {subsection}{\numberline {11.8.1}Approximation}{342}{subsection.11.8.1}% 
\contentsline {subsection}{\numberline {11.8.2}Best Low Rank Approximation - Proof}{344}{subsection.11.8.2}% 
\contentsline {subsection}{\numberline {11.8.3}SVD - Complessit\IeC {\`a}}{344}{subsection.11.8.3}% 
\contentsline {section}{\numberline {11.9}Confronto tra PCA e SVD}{345}{section.11.9}% 
\contentsline {section}{\numberline {11.10}Matrix Factorization}{346}{section.11.10}% 
\contentsline {subsection}{\numberline {11.10.1}L'esempio di Netflix}{346}{subsection.11.10.1}% 
\contentsline {subsubsection}{Alternating Optimization}{349}{section*.554}% 
\contentsline {subsubsection}{Overfitting}{353}{section*.564}% 
\contentsline {subsubsection}{Risolvere l'overfitting con la Regolarizzazione}{353}{section*.565}% 
\contentsline {subsubsection}{L2 vs L1 regularization}{355}{section*.568}% 
\contentsline {section}{\numberline {11.11}Altri metodi per fattorizzare le matrici}{356}{section.11.11}% 
\contentsline {subsubsection}{Boolean Matrix factorization}{357}{section*.571}% 
\contentsline {section}{\numberline {11.12}AutoEncoders}{358}{section.11.12}% 
\contentsline {chapter}{\numberline {12}Clustering}{363}{chapter.12}% 
\contentsline {section}{\numberline {12.1}K-Means}{363}{section.12.1}% 
\contentsline {subsubsection}{Come scegliamo la K?}{366}{section*.578}% 
\contentsline {subsubsection}{KMeans++}{367}{section*.579}% 
\contentsline {subsubsection}{Problemi}{367}{section*.580}% 
\contentsline {section}{\numberline {12.2}Guassian Mixture Model}{368}{section.12.2}% 
\contentsline {subsubsection}{Stimare i parametri}{368}{section*.581}% 
\contentsline {subsubsection}{Gaussian Mixture Model}{369}{section*.582}% 
\contentsline {subsubsection}{Inference Phase}{372}{section*.589}% 
\contentsline {subsubsection}{Learning Phase}{374}{section*.593}% 
\contentsline {section}{\numberline {12.3}Expectation Maximization}{378}{section.12.3}% 
\contentsline {chapter}{\numberline {13}Introduction to PyTorch}{383}{chapter.13}% 
\contentsline {section}{\numberline {13.1}Manually Setup the Affine Layer}{383}{section.13.1}% 
\contentsline {section}{\numberline {13.2}Declare the layer using PyTorch}{384}{section.13.2}% 
\contentsline {subsection}{\numberline {13.2.1}nn.Linear}{385}{subsection.13.2.1}% 
\contentsline {subsection}{\numberline {13.2.2}Torch.optim}{385}{subsection.13.2.2}% 
\contentsline {subsection}{\numberline {13.2.3}DataLoader}{386}{subsection.13.2.3}% 
\contentsline {section}{\numberline {13.3}Validation}{386}{section.13.3}% 
\contentsline {section}{\numberline {13.4}Implement a CNN using PyTorch}{386}{section.13.4}% 
\contentsline {subsection}{\numberline {13.4.1}nn.Sequential}{387}{subsection.13.4.1}% 
\contentsline {chapter}{\numberline {14}Math Refresh}{389}{chapter.14}% 
\contentsline {section}{\numberline {14.1}Loss function}{389}{section.14.1}% 
\contentsline {section}{\numberline {14.2}0-1 Loss}{389}{section.14.2}% 
\contentsline {section}{\numberline {14.3}Lagrange Multipliers}{391}{section.14.3}% 
\contentsline {subsubsection}{Lagrange multiplier con vincoli $\geq $}{392}{section*.616}% 
\contentsline {section}{\numberline {14.4}Marginalisation}{393}{section.14.4}% 
\contentsline {section}{\numberline {14.5}Probability Distribution}{394}{section.14.5}% 
\contentsline {subsection}{\numberline {14.5.1}Bernoulli Distribution}{394}{subsection.14.5.1}% 
\contentsline {subsection}{\numberline {14.5.2}Normal Distribution}{395}{subsection.14.5.2}% 
\contentsline {subsection}{\numberline {14.5.3}Beta Distribution}{396}{subsection.14.5.3}% 
\contentsline {subsection}{\numberline {14.5.4}Categorical Distribution}{396}{subsection.14.5.4}% 
\contentsline {section}{\numberline {14.6}Propriet\IeC {\`a} dei logaritmi}{397}{section.14.6}% 
\contentsline {section}{\numberline {14.7}Derivate}{397}{section.14.7}% 
\contentsline {section}{\numberline {14.8}Integrali}{398}{section.14.8}% 
\contentsline {section}{\numberline {14.9}Derivate di matrici e Vettori}{399}{section.14.9}% 
\contentsline {section}{\numberline {14.10}Visualizzare le distribuzioni delle probabilit\IeC {\`a}}{400}{section.14.10}% 
\contentsline {section}{\numberline {14.11}Entropia}{402}{section.14.11}% 
